# Databricks Spark Project: Census Income Dataset Analysis

Welcome to the Databricks Spark project focused on the Census Income dataset analysis. This project showcases the power of Apache Spark and its applications in data analysis and machine learning.

## Project Overview

In this project, we used Databricks and Apache Spark to perform a comprehensive analysis of the Census Income dataset. Our goal was to gain valuable insights into income prediction while also getting hands-on experience with key data science tools and techniques.

## Project Highlights

- **Data Preprocessing**: We employed PySpark for data preprocessing tasks, ensuring that the dataset was clean and ready for analysis.

- **Machine Learning Model**: To predict income levels, we implemented a Logistic Regression model. This step marked our entry into the world of Spark-based machine learning.

- **Data Visualization**: Visualizing data trends and patterns was a crucial part of our analysis. We leveraged various visualization techniques to make data more accessible and informative.

- **MLflow Integration**: To maintain a clear record of our project's progress, we utilized MLflow for experiment tracking. This helped us log and monitor the performance of our machine learning model.

- **Achieved 84% Accuracy**: Our hard work and dedication paid off when our model achieved an accuracy rate of 84% in predicting income levels.

## Repository Structure

- `data/`: Contains the dataset used for analysis.
- `notebooks/`: Includes Jupyter notebooks documenting our step-by-step analysis.
- `models/`: Stores the trained machine learning model.
- `visualizations/`: Holds the visualizations generated during data exploration.
- `README.md`: The file you are currently reading, providing an overview of the project.

## Getting Started

To reproduce or further explore this project, follow these steps:

1. Clone this repository to your local machine using `git clone`.

2. Set up a Databricks environment with Apache Spark.

3. Open and run the Jupyter notebooks in the `notebooks/` directory to explore the project's analysis.

4. Check out the `models/` directory to access the trained machine learning model.

5. Review the `visualizations/` directory for insights gained through data visualization.

## Dependencies

- Apache Spark
- PySpark
- MLflow
- Databricks Notebook

## Contributor

- Devavrat Samak

## Acknowledgments

We'd like to express our gratitude to the Databricks community and all those who have contributed to the tools and libraries that made this project possible.
